{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5949c472-e095-42e1-975a-45f15bf284ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "df = spark.read.csv(\"/Volumes/workspace/default/pyspark/customer_details.csv\", header=True, inferSchema=True)\n",
    "#df.select(\"name\", \"salary\").show()\n",
    "df.show()\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61bcdff-3316-4c6d-ab9f-da9c32fc1b1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customer = spark.sql(\"select * from workspace.default.customer_details\")\n",
    "\n",
    "\n",
    "df_customer.show()\n",
    "display(df_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644233eb-f352-4f0e-87cd-c8aa15e15eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JoinExamples\").getOrCreate()\n",
    "\n",
    "# Create df1\n",
    "data1 = [\n",
    "    Row(id=1, name=\"Alice\"),\n",
    "    Row(id=2, name=\"Bob\"),\n",
    "    Row(id=3, name=\"Carol\"),\n",
    "    Row(id=4, name=\"David\")\n",
    "]\n",
    "df1 = spark.createDataFrame(data1)\n",
    "\n",
    "# Create df2\n",
    "data2 = [\n",
    "    Row(id=2, salary=50000),\n",
    "    Row(id=3, salary=60000),\n",
    "    Row(id=5, salary=70000)\n",
    "]\n",
    "df2 = spark.createDataFrame(data2)\n",
    "\n",
    "df1.show()\n",
    "df1.display()\n",
    "\n",
    "df2.show()\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94001bfa-ec49-4a4c-b018-700d6b0a2f70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Count of Data: \" ,df_customer.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a988fc-a8a0-4b68-bbd4-2aa67d554d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df_customer.printSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eeb360e-c394-4dbf-b10c-7715c12017bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, FloatType\n",
    "from datetime import date\n",
    "\n",
    "# 1. Start Spark Session\n",
    "spark = SparkSession.builder.appName(\"EmployeeDataFrame\").getOrCreate()\n",
    "\n",
    "# 2. Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"joining_date\", DateType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"experience\", FloatType(), True),\n",
    "])\n",
    "\n",
    "# 3. Create Data\n",
    "data = [\n",
    "    (101, \"Amit Sharma\", \"Engineering\", 95000, date(2018, 7, 15), \"Bangalore\", 6.2),\n",
    "    (102, \"Neha Verma\", \"Marketing\", 72000, date(2020, 3, 1), \"Delhi\", 4.0),\n",
    "    (103, \"Rohan Mehta\", \"Sales\", 68000, date(2019, 11, 20), \"Mumbai\", 5.1),\n",
    "    (104, \"Divya Nair\", \"HR\", 60000, date(2021, 6, 18), \"Chennai\", 3.0),\n",
    "    (105, \"Sameer Khan\", \"Engineering\", 105000, date(2017, 9, 10), \"Bangalore\", 7.8),\n",
    "    (106, \"Isha Malhotra\", \"Finance\", 85000, date(2022, 1, 12), \"Hyderabad\", 2.5),\n",
    "]\n",
    "\n",
    "# 4. Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# 5. Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# 6. Save DataFrame as CSV (multiple part files)\n",
    "df.write.mode(\"overwrite\").option(\"header\", True).csv(\"/Volumes/workspace/default/pyspark/employee_details.csv\")\n",
    "\n",
    "# Optional: Save as a single CSV file\n",
    "# df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"dbfs:/FileStore/employee_details_csv_single\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c212179-7442-4783-a7aa-4e64184c57d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/Volumes/workspace/default/pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "814b16a3-314f-46ae-a508-4e6c2586fbd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"/Volumes/workspace/default/pyspark/customer_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6059386-5366-4523-9d76-2bc74cfc583b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark 1.0",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
